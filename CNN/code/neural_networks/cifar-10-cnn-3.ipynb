{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8172589,"sourceType":"datasetVersion","datasetId":4837039},{"sourceId":10588448,"sourceType":"datasetVersion","datasetId":6553089}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-13T12:03:30.867058Z","iopub.execute_input":"2025-02-13T12:03:30.867364Z","iopub.status.idle":"2025-02-13T12:03:31.186360Z","shell.execute_reply.started":"2025-02-13T12:03:30.867341Z","shell.execute_reply":"2025-02-13T12:03:31.185637Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/test-cifar100/test\n/kaggle/input/cifar10-test-data-sp24-npy/cifar10_test_data_sp24.npy\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Imports for pytorch\nimport numpy as np\nimport torch\nimport torchvision\nfrom torch import nn\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport tqdm\nimport copy\nimport torch.utils.data as data\n\nSEED = 1234\n\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T12:03:31.187381Z","iopub.execute_input":"2025-02-13T12:03:31.187990Z","iopub.status.idle":"2025-02-13T12:03:33.754786Z","shell.execute_reply.started":"2025-02-13T12:03:31.187954Z","shell.execute_reply":"2025-02-13T12:03:33.753859Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"transform = torchvision.transforms.ToTensor()\n\nunprocessed_train_data = torchvision.datasets.CIFAR100(\n    root=\"data\",\n    train=True,\n    download=True\n)\n\n\"\"\"\n    \n                           torchvision.transforms.ColorJitter(brightness = 0.1,\n                                                              contrast = 0.1, \n                                                              saturation = 0.1), \n                           torchvision.transforms.RandomAdjustSharpness(sharpness_factor = 2,\n                                                                        p = 0.1),\n                           torchvision.transforms.RandomErasing(p=0.75,\n                                                                scale=(0.02, 0.1),\n                                                                value=1.0, \n                                                                inplace=False),\n                            \"\"\"\n\n\narr_mean = np.mean(unprocessed_train_data.data, axis=(0, 1, 2)) / 255\narr_sd = np.std(unprocessed_train_data.data, axis=(0, 1, 2)) / 255\n\nSIZE = 112\n\n\ntrain_transforms = torchvision.transforms.Compose([\n                           torchvision.transforms.Resize(SIZE),\n                           torchvision.transforms.RandomRotation(20),\n                           torchvision.transforms.RandomHorizontalFlip(0.5),\n                           torchvision.transforms.RandomCrop(SIZE, padding=10),\n                           torchvision.transforms.ColorJitter(brightness = 0.1,\n                                                              contrast = 0.1, \n                                                              saturation = 0.1), \n                           torchvision.transforms.RandomAdjustSharpness(sharpness_factor = 2,\n                                                                        p = 0.1),\n                           \n                           torchvision.transforms.ToTensor(),\n                           torchvision.transforms.Normalize(mean=arr_mean,\n                                                std=arr_sd)\n                       ])\n\n\ntest_transforms = torchvision.transforms.Compose([\n                           torchvision.transforms.Resize(SIZE),\n                           torchvision.transforms.ToTensor(),\n                           torchvision.transforms.Normalize(mean=arr_mean,\n                                                std=arr_sd)\n                           ])\n\ntraining_data = torchvision.datasets.CIFAR10(root = 'data',\n                              train=True,\n                              download=True,\n                              transform=train_transforms)\n\ntest_data = torchvision.datasets.CIFAR10(root = 'data',\n                             train=False,\n                             download=True,\n                             transform=test_transforms)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T12:03:33.756600Z","iopub.execute_input":"2025-02-13T12:03:33.756958Z","iopub.status.idle":"2025-02-13T12:03:37.446529Z","shell.execute_reply.started":"2025-02-13T12:03:33.756936Z","shell.execute_reply":"2025-02-13T12:03:37.445851Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"test_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T12:03:37.447610Z","iopub.execute_input":"2025-02-13T12:03:37.447821Z","iopub.status.idle":"2025-02-13T12:03:37.453692Z","shell.execute_reply.started":"2025-02-13T12:03:37.447794Z","shell.execute_reply":"2025-02-13T12:03:37.452848Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Dataset CIFAR10\n    Number of datapoints: 10000\n    Root location: data\n    Split: Test\n    StandardTransform\nTransform: Compose(\n               Resize(size=112, interpolation=bilinear, max_size=None, antialias=True)\n               ToTensor()\n               Normalize(mean=[0.50707516 0.48654887 0.44091784], std=[0.26733429 0.25643846 0.27615047])\n           )"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"train_data, val_data = data.random_split(training_data,\n                                           [int(training_data.data.shape[0] * 0.9), int(training_data.data.shape[0] * 0.1)])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T12:03:37.454558Z","iopub.execute_input":"2025-02-13T12:03:37.454762Z","iopub.status.idle":"2025-02-13T12:03:37.468267Z","shell.execute_reply.started":"2025-02-13T12:03:37.454744Z","shell.execute_reply":"2025-02-13T12:03:37.467506Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"val_data = copy.deepcopy(val_data)\nval_data.dataset.transform = test_transforms #make sure the val data uses test transform","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T12:03:37.469182Z","iopub.execute_input":"2025-02-13T12:03:37.469407Z","iopub.status.idle":"2025-02-13T12:03:37.555876Z","shell.execute_reply.started":"2025-02-13T12:03:37.469388Z","shell.execute_reply":"2025-02-13T12:03:37.554930Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# **Build Model Architecture**\n","metadata":{}},{"cell_type":"code","source":"class NormalizedLayer(nn.Module):\n    def __init__(self, size):\n        super().__init__()\n        self.linear1 = nn.Linear(size, size)\n        self.linear2 = nn.Linear(size, size)\n        self.layer = nn.RMSNorm(size)\n        self.dropout = nn.Dropout(0.4)\n\n\n    def forward(self, input):\n        x = input\n        x = self.dropout(x)\n        x = self.linear1(x)\n        x = nn.functional.relu(x)\n        x = torch.square(x)\n        x = self.layer(x)\n        x = self.linear2(x)\n        x = x + input\n        return x\n\nclass Classifier(nn.Module):\n    def __init__(self, output_dim, hidden_dim = 512, layer_count = 32):\n        super().__init__()\n\n        self.dropout = nn.Dropout(.5)\n        self.reduction = nn.Linear(25088, hidden_dim)\n        #self.reduction = nn.Linear(6272, hidden_dim)\n        self.layer = nn.RMSNorm(hidden_dim)\n        \n        self.layers = []\n        for i in range(layer_count):\n            layer = NormalizedLayer(hidden_dim)\n            self.layers.append(layer)\n            self.register_module('layer' + str(i), layer)\n        self.linearOutput = nn.Linear(hidden_dim, output_dim)\n        \n\n    def forward(self, x):\n\n        x = self.dropout(x)\n        x = self.reduction(x)\n        x = self.layer(x)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        x = self.linearOutput(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2025-02-13T12:03:37.556719Z","iopub.execute_input":"2025-02-13T12:03:37.556945Z","iopub.status.idle":"2025-02-13T12:03:37.564543Z","shell.execute_reply.started":"2025-02-13T12:03:37.556926Z","shell.execute_reply":"2025-02-13T12:03:37.563530Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\n\nclass VGG(nn.Module):\n    def __init__(self, features, output_dim):\n        super().__init__()\n\n        self.features = features\n\n        self.avgpool = nn.AdaptiveAvgPool2d((7,7))\n\n        self.classifier = Classifier(output_dim)\n    \n\n    def forward(self, x):\n        x = self.features(x)\n        #x = self.avgpool(x)\n        h = x.view(x.shape[0], -1)\n        x = self.classifier(h)\n        return x, h\n\n \n                \ndef design_layers(config, batch_norm):\n\n    layers = []\n    in_channels = 3\n\n    for c in config:\n\n        if c == 'M':\n            #layers += [nn.Conv2d(in_channels, in_channels, kernel_size=2, stride = 2)]\n            layers += [nn.MaxPool2d(kernel_size=2, stride = 2)]\n\n        else:\n        \n            conv2d = nn.Conv2d(in_channels, c, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(c), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = c\n\n    return nn.Sequential(*layers)\nseq = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2025-02-13T12:03:37.566951Z","iopub.execute_input":"2025-02-13T12:03:37.567159Z","iopub.status.idle":"2025-02-13T12:03:37.582022Z","shell.execute_reply.started":"2025-02-13T12:03:37.567140Z","shell.execute_reply":"2025-02-13T12:03:37.581231Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"\"\\n64, 64, 'M', \\n       64, 64, 'M',  \\n       64, 64, 'M',  \\n       64, 512, 'M']\\n\""},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"vgg1_layers = design_layers(seq, batch_norm=True)\nmodel = VGG(vgg1_layers, 10)\nvgg1_layers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T12:03:37.583102Z","iopub.execute_input":"2025-02-13T12:03:37.583340Z","iopub.status.idle":"2025-02-13T12:03:37.861865Z","shell.execute_reply.started":"2025-02-13T12:03:37.583319Z","shell.execute_reply":"2025-02-13T12:03:37.861071Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Sequential(\n  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU(inplace=True)\n  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (6): ReLU(inplace=True)\n  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (8): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (10): ReLU(inplace=True)\n  (11): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (13): ReLU(inplace=True)\n  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (15): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (17): ReLU(inplace=True)\n  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (20): ReLU(inplace=True)\n  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"BATCH_SIZE = 64\n\ntraining_data = torchvision.datasets.CIFAR10(root = 'data',\n                              train=True,\n                              download=True,\n                              transform=train_transforms)\n\n\ntrain_iterator = data.DataLoader(train_data,\n                                 shuffle=True,\n                                 batch_size=BATCH_SIZE)\n\nval_iterator = data.DataLoader(val_data,\n                                 batch_size=BATCH_SIZE)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T12:03:37.862627Z","iopub.execute_input":"2025-02-13T12:03:37.862876Z","iopub.status.idle":"2025-02-13T12:03:38.778668Z","shell.execute_reply.started":"2025-02-13T12:03:37.862854Z","shell.execute_reply":"2025-02-13T12:03:38.777988Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nmodel = model.to(device)\nCEL = nn.CrossEntropyLoss().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T12:03:38.779354Z","iopub.execute_input":"2025-02-13T12:03:38.779595Z","iopub.status.idle":"2025-02-13T12:03:38.964359Z","shell.execute_reply.started":"2025-02-13T12:03:38.779573Z","shell.execute_reply":"2025-02-13T12:03:38.963416Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import torch.optim as optim\n\n\nparams = [\n          {'params': model.features.parameters()}, #, 'lr': 5e-4 / 10},\n          {'params': model.classifier.parameters()}\n         ]\n\noptimizer = optim.AdamW(params, lr= 5e-3, weight_decay = 1e-4)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T12:03:38.965279Z","iopub.execute_input":"2025-02-13T12:03:38.965546Z","iopub.status.idle":"2025-02-13T12:03:38.970411Z","shell.execute_reply.started":"2025-02-13T12:03:38.965517Z","shell.execute_reply":"2025-02-13T12:03:38.969518Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from tqdm.notebook import trange, tqdm\n\ndef train(model, iterator, optimizer, CEL, device):\n\n    epoch_loss = 0\n    epoch_accuracy = 0\n\n    model.train()\n\n    for (x, y) in tqdm(iterator, desc=\"Training\", leave=False):\n\n        x = x.to(device)\n        y = y.to(device)\n\n        optimizer.zero_grad()\n\n        y_pred,_ = model(x)\n\n        loss = CEL(y_pred, y)\n\n        accuracy = calculate_accuracy(y_pred, y)\n\n        loss.backward()\n\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        epoch_accuracy += accuracy.item()\n\n    return epoch_loss / len(iterator), epoch_accuracy / len(iterator)\n\n\ndef evaluate(model, iterator, CEL, device):\n\n    epoch_loss = 0\n    epoch_accuracy = 0\n\n    model.eval()\n\n    with torch.no_grad():\n\n        for (x, y) in tqdm(iterator, desc=\"Evaluating\", leave=False):\n\n            x = x.to(device)\n            y = y.to(device)\n\n            y_pred,_ = model(x)\n\n            loss = CEL(y_pred, y)\n\n            accuracy = calculate_accuracy(y_pred, y)\n\n            epoch_loss += loss.item()\n            epoch_accuracy += accuracy.item()\n\n    return epoch_loss / len(iterator), epoch_accuracy / len(iterator)\n\ndef calculate_accuracy(y_pred, y):\n    first_pred = y_pred.argmax(1, keepdim=True)\n    correct = first_pred.eq(y.view_as(first_pred)).sum()\n    accuracy = correct.float() / y.shape[0]\n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T12:03:38.971318Z","iopub.execute_input":"2025-02-13T12:03:38.971650Z","iopub.status.idle":"2025-02-13T12:03:39.047767Z","shell.execute_reply.started":"2025-02-13T12:03:38.971619Z","shell.execute_reply":"2025-02-13T12:03:39.047097Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"EPOCHS = 20\nBATCH_SIZE = 64\n\n\nbest_val_loss = float('inf')\n\ntrain_accuracies = []\nval_accuracies = []\ntrain_losses = []\nval_losses = []\n\nval_iterator = data.DataLoader(val_data,\n                                 batch_size=BATCH_SIZE)\n\nfor epoch in trange(EPOCHS):\n    \n    training_data = torchvision.datasets.CIFAR10(root = 'data',\n                              train=True,\n                              download=False,\n                              transform=train_transforms)\n    train_iterator = data.DataLoader(train_data,\n                                 shuffle=True,\n                                 batch_size=BATCH_SIZE)\n\n    train_loss, train_accuracy = train(model, train_iterator, optimizer, CEL, device)\n    val_loss, val_accuracy = evaluate(model, val_iterator, CEL, device)\n\n    train_accuracies.append(train_accuracy)\n    val_accuracies.append(val_accuracy)\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), 'tut1-model.pt')\n\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_accuracy*100:.2f}%')\n    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_accuracy*100:.2f}%')\n\n\nplt.plot(range(1, EPOCHS + 1), train_accuracies, label='Train Accuracy')\nplt.plot(range(1, EPOCHS + 1), val_accuracies, label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Train/Validation Accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(range(1, EPOCHS + 1), train_losses, label='Train Loss')\nplt.plot(range(1, EPOCHS + 1), val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Train/Validation Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T12:03:39.048381Z","iopub.execute_input":"2025-02-13T12:03:39.048614Z","execution_failed":"2025-02-13T13:12:12.135Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a98404b477794edb9a185ee1f7c68fec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/704 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/79 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Loss: 4.472 | Train Acc: 24.35%\n\t Val. Loss: 1.806 |  Val. Acc: 35.82%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/704 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/79 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Loss: 1.413 | Train Acc: 48.20%\n\t Val. Loss: 1.093 |  Val. Acc: 60.27%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/704 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/79 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Loss: 1.057 | Train Acc: 62.27%\n\t Val. Loss: 0.903 |  Val. Acc: 68.43%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/704 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/79 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Loss: 0.889 | Train Acc: 68.67%\n\t Val. Loss: 0.731 |  Val. Acc: 74.74%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/704 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/79 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Loss: 0.784 | Train Acc: 72.58%\n\t Val. Loss: 0.628 |  Val. Acc: 78.03%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/704 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/79 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Loss: 0.714 | Train Acc: 75.36%\n\t Val. Loss: 0.646 |  Val. Acc: 77.77%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/704 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/79 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Loss: 0.655 | Train Acc: 77.27%\n\t Val. Loss: 0.639 |  Val. Acc: 77.97%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/704 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/79 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\tTrain Loss: 0.616 | Train Acc: 78.57%\n\t Val. Loss: 0.597 |  Val. Acc: 78.84%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/704 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"298c99fd739e489385b6539aeebe51ae"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport os\n\nclass CIFAR10Test(torchvision.datasets.VisionDataset):\n\n    def __init__(self, transform=None, target_transform=None):\n        super(CIFAR10Test, self).__init__(None, transform=transform,\n                                      target_transform=target_transform)\n        assert os.path.exists(\"/kaggle/input/cifar10-test-data-sp24-npy/cifar10_test_data_sp24.npy\"), \"You must upload the test data to the file system.\"\n        self.data = [np.load(\"/kaggle/input/cifar10-test-data-sp24-npy/cifar10_test_data_sp24.npy\", allow_pickle=False)]\n\n        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)\n        self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n\n    def __getitem__(self, index: int):\n        img = self.data[index]\n        img = Image.fromarray(img)\n        if self.transform is not None:\n            img = self.transform(img)\n        return img\n\n    def __len__(self) -> int:\n        return len(self.data)\n\n# Create the test dataset\ntesting_data = CIFAR10Test(\n    transform=test_transforms # NOTE: Make sure transform is the same as used in the training dataset.\n)\n\ntest_data = torchvision.datasets.CIFAR10(root = 'data',\n                             train=False,\n                             download=True,\n                             transform=test_transforms)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-13T13:12:12.171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_iterator = data.DataLoader(testing_data,\n                                batch_size=BATCH_SIZE,\n                               shuffle = False)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-13T13:12:12.172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-13T13:12:12.173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef get_predictions(model, iterator):\n\n    model.eval()\n    model = model.to(device)\n    labels = []\n\n    with torch.no_grad():\n        for x in tqdm(iterator):\n            x = x.to(device)\n\n            y_pred, _ = model(x)\n            \n            _, predicted_labels = torch.max(y_pred, 1)\n            \n            labels.extend(predicted_labels.tolist())\n    \n\n    return labels\n\n\npredictions = get_predictions(model, test_iterator)\nlen(predictions)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-13T13:12:12.175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\nif isinstance(predictions, np.ndarray):\n    predictions = predictions.astype(int)\nelse:\n    predictions = np.array(predictions, dtype=int)\nassert predictions.shape == (len(testing_data),), \"Predictions were not the correct shape\"\ndf = pd.DataFrame({'Category': predictions})\ndf.index += 1  # Ensures that the index starts at 1.\ndf.to_csv('submission.csv', index_label='Id')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-13T13:12:12.176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def unpickle(file):\n    import pickle\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='bytes')\n    return dict\ndf","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-13T13:12:12.186Z"}},"outputs":[],"execution_count":null}]}